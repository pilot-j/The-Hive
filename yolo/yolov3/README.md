Yolov3 
To understand the architecture of this version we need to understand the challenges faced by previous versions(yolo, yolo9000).
Previous versions depended on convolutional/maxpooling layers only to generate feature maps. While this s a good way of retaining important features
it usually *averages the fine details of any image*. This is evident by low performance of such backbones(Darknet 19) on small object detection. 

So, we needed **some changes to retain details and extract rich feature maps.** One way around this was to introduce skip connections. Another trick can be
to avoid unnecessary averaging i.e. to remove pooling layers. This give rise to Darknet 53 ---> CBL + RESIDUALBlocks + minimal pooling !!

1) How to downsample without pooling? ---> Use convolutional with varying strides. For eg --> 2*2 filter, stride =2. This offers 2 benefits - abstraction of feature is better than averaging and the filters are learned params.
2) What does the model predict anyway? ---> Our aim is to localise and classify objects. So, intuitively the model should predict a box and also assign it some probability as to which class can the object belong. Say, your dataset has 15 classes, so the output should have a 4+1+15 = 20 dimensional vector as output. 4 points to define a box, 1 indicator variable to see if the box contains any object or not and a hot endcoded vector for respective class. YOLO provides one prediction for each grid cell, and each grid cell here is associated with 3 anchors(more about them later). So, we actually output N*N * (3*(4+1+num_classes)).
3) Levels of predictions ---> This model solves tries to solve the different scale problem by generating 3 different sized maps for different size predictions. Small abstract maps can be used to detect large objects, medium sized maps will contain clues to medium sized objects while large sized rich maps can be used to detect small objects. This intuitively explains the presence of 13*13, 26*26, 52*52  maps where predictions are made. On carefully observing, we can see skip connections from early layers are used to form 52*52 map which hence retain lot of details compared to smalled maps generated before it.
4) How many predictions does the model make? ---> This model predicts at 3 different levels, generating 10k + bboxes. This is about 10 times yolo9000.
5) Multi Label Classifictaion ---> We are so used to seeing softmax as activation function that sometimes it is a good mental exercise to reflect on its utility. _Softmax essentially allows us to interpret random distribution of numbers between (0,1) as a probability distribution_. Thus softmax enforces ****multi** **classification** **wherein stronger presence of class 1 will automatically indicate weaker presence of class2. However, is there a way to assign multiple labels to a single object? For eg. --> A kathak dancer can be classified both as a dancer and as a person. To help with this, authors of the paper use sigmoid to ensure each output is between(0,1) but not normalised. This way we can play around with multi labels. 
