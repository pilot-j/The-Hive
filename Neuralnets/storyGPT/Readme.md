# TODO.md

### Todo

- [x] Add executable notebook file  
- [ ] Training on full dataset (don't have enough GPU access right now)
- [ ] Add python files that can be executed on system (under progress)
- [ ] Create web app that can take user prompt and generate short story using StreamLit (under progress)
  

# GPT-Inspired Story Generator

## Introduction

This project is an attempt into building a generative pre-trained language model inspired by the principles of GPT (Generative Pre-trained Transformer). This is a character-level autoregressive model capable of generating coherent English text. Here, I take the opportunity to reproduce(and test) the claims of this interesting research paper ["TinyStories: How Small Can Language Models Be and Still Speak Coherent English?"](https://arxiv.org/pdf/2305.07759.pdf).

The initial model is tested agasint a subset of the original TinyStories dataset. The TinyStories dataset is designed to investigate the minimal requirements for language models to produce coherent and meaningful English text.

## Acknowledgments

This project draws inspiration from [Andrej Karpathy's nanoGPT](https://youtu.be/kCc8FmEb1nY?feature=shared) and the informative blog post on GPT-2 by [Jay Alammar](https://jalammar.github.io/illustrated-gpt2/). The guidance provided in these resources has been instrumental in shaping the direction of this language model.

---

**Note:** This README is a work in progress and will be refined further as the project evolves. Contributions, feedback, and collaboration are welcome!


