{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 20000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 128\n",
        "n_head = 16\n",
        "n_layer = 8\n",
        "dropout = 0.1\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "with open('/content/Tiny_stories.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "text = text.replace(\"<|endoftext|>\", \"#\")\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoelkOrFY8bN",
        "outputId": "b5c97429-806d-4e8f-d254-032d7253880a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.60853 M parameters\n",
            "step 0: train loss 4.4828, val loss 4.4875\n",
            "step 300: train loss 2.0922, val loss 2.1227\n",
            "step 600: train loss 1.7916, val loss 1.8404\n",
            "step 900: train loss 1.6032, val loss 1.6830\n",
            "step 1200: train loss 1.5204, val loss 1.5859\n",
            "step 1500: train loss 1.4493, val loss 1.5286\n",
            "step 1800: train loss 1.4034, val loss 1.4847\n",
            "step 2100: train loss 1.3826, val loss 1.4566\n",
            "step 2400: train loss 1.3346, val loss 1.4170\n",
            "step 2700: train loss 1.3349, val loss 1.4067\n",
            "step 3000: train loss 1.2949, val loss 1.3825\n",
            "step 3300: train loss 1.2871, val loss 1.3737\n",
            "step 3600: train loss 1.2606, val loss 1.3423\n",
            "step 3900: train loss 1.2685, val loss 1.3395\n",
            "step 4200: train loss 1.2396, val loss 1.3125\n",
            "step 4500: train loss 1.2288, val loss 1.2973\n",
            "step 4800: train loss 1.2218, val loss 1.2935\n",
            "step 5100: train loss 1.2039, val loss 1.2807\n",
            "step 5400: train loss 1.2028, val loss 1.2913\n",
            "step 5700: train loss 1.1943, val loss 1.2797\n",
            "step 6000: train loss 1.1827, val loss 1.2598\n",
            "step 6300: train loss 1.1838, val loss 1.2516\n",
            "step 6600: train loss 1.1602, val loss 1.2606\n",
            "step 6900: train loss 1.1628, val loss 1.2501\n",
            "step 7200: train loss 1.1478, val loss 1.2519\n",
            "step 7500: train loss 1.1513, val loss 1.2419\n",
            "step 7800: train loss 1.1502, val loss 1.2496\n",
            "step 8100: train loss 1.1371, val loss 1.2181\n",
            "step 8400: train loss 1.1308, val loss 1.2080\n",
            "step 8700: train loss 1.1314, val loss 1.2262\n",
            "step 9000: train loss 1.1326, val loss 1.2223\n",
            "step 9300: train loss 1.1124, val loss 1.2082\n",
            "step 9600: train loss 1.1129, val loss 1.2140\n",
            "step 9900: train loss 1.1167, val loss 1.2020\n",
            "step 10200: train loss 1.0993, val loss 1.2048\n",
            "step 10500: train loss 1.1123, val loss 1.1900\n",
            "step 10800: train loss 1.0897, val loss 1.1982\n",
            "step 11100: train loss 1.0897, val loss 1.1971\n",
            "step 11400: train loss 1.0903, val loss 1.1917\n",
            "step 11700: train loss 1.0816, val loss 1.1806\n",
            "step 12000: train loss 1.0877, val loss 1.1777\n",
            "step 12300: train loss 1.0905, val loss 1.1948\n",
            "step 12600: train loss 1.0889, val loss 1.1935\n",
            "step 12900: train loss 1.0772, val loss 1.1780\n",
            "step 13200: train loss 1.0871, val loss 1.1801\n",
            "step 13500: train loss 1.0696, val loss 1.1752\n",
            "step 13800: train loss 1.0782, val loss 1.1653\n",
            "step 14100: train loss 1.0739, val loss 1.1805\n",
            "step 14400: train loss 1.0688, val loss 1.1601\n",
            "step 14700: train loss 1.0587, val loss 1.1793\n",
            "step 15000: train loss 1.0822, val loss 1.1674\n",
            "step 15300: train loss 1.0759, val loss 1.1663\n",
            "step 15600: train loss 1.0636, val loss 1.1702\n",
            "step 15900: train loss 1.0641, val loss 1.1736\n",
            "step 16200: train loss 1.0558, val loss 1.1644\n",
            "step 16500: train loss 1.0570, val loss 1.1581\n",
            "step 16800: train loss 1.0477, val loss 1.1553\n",
            "step 17100: train loss 1.0523, val loss 1.1624\n",
            "step 17400: train loss 1.0447, val loss 1.1617\n",
            "step 17700: train loss 1.0507, val loss 1.1589\n",
            "step 18000: train loss 1.0510, val loss 1.1505\n",
            "step 18300: train loss 1.0466, val loss 1.1582\n",
            "step 18600: train loss 1.0468, val loss 1.1458\n",
            "step 18900: train loss 1.0516, val loss 1.1557\n",
            "step 19200: train loss 1.0425, val loss 1.1414\n",
            "step 19500: train loss 1.0427, val loss 1.1498\n",
            "step 19800: train loss 1.0329, val loss 1.1422\n",
            "step 19999: train loss 1.0294, val loss 1.1374\n",
            "\n",
            "Tom looked at at the carfbood and solve it was anted that it was the lung can were right.\n",
            "Once upon a time, there was a pracee and vowed which it was time.\n",
            "One day, Jenny quakened a beautiful close only near it?\"\n",
            "The book spoiled and they all \n",
            "\"Come home, but! That is matiches, but he had a great careful, mean put the slowly wing eventually! Joe had so much fun outside with John's not wait to patchen. Jane seemed the car for some money in the brown. It was brange, he should swing very much. He could not fit like them gained the end, Cooldil it found I'm for his gianic and could inside his dinosappointed what his mom as fair what to do this markiat stream to have fun it.\n",
            "But then around her placing gazed.\n",
            "Everywhere \"I stole when I got insister\" as she reaching. He helped playing with his mom. He loved to eat people, so thaught you for your foat!\" \n",
            "Sam smiled. She was big many whiled with the big feeling better and they find a late puddle ness, and a little girl named Lucy.\n",
            "They were tried to sorver.\n",
            "When the other opened the living around, became inseed of the po!\" the friend said he she wanted it tound his sackyard. Once day, a bild filled with her mommy. \n",
            "\"Where dog, she was here playing with yours farmhing.\"\n",
            "Ben stopped never and his friend! But when it grabbed it together. \n",
            "One day, Jane came with and away.\n",
            "Even time when she was being and played.\n",
            "\"Mom, I will say yue that! Max let it has crazing.\"\n",
            "The lest one day, Bamy started to go together and had the kept right. The rish was so excited! She was excited something for a steam right. Tweetiar, it is time. It had together and blocks.‚Äù\n",
            "As they found a hole and need to take the storm laughed.\n",
            "They promised to share it was biking and thanked to be mystelf. \"But, thanks I lost yourself,!\" is Lily asking inside. He looks at, Tim missed them. He wild not you go away or their bikes than Tom and Ben had a speck afeely.\n",
            "Once play and waited too, and playing with the wunch. After a few minuted, the bird snowman wanted to\n"
          ]
        }
      ]
    }
  ]
}